# WorldBench

Official code repository for (WorldBench: Quantifying Geographic Disparities in LLM Factual Recall)[https://dl.acm.org/doi/10.1145/3630106.3658967] (FAccT 2024). 

Probing the ability of language models to answer factual questions about countries around the world, towards uncovering and quantifying disparities based on geography or question content. 

Coming soon: code clean up, demo. The file `query.py` shows an example of how you can evaluate a language model on all 11 indicators studied in the paper.

# Citation

```
@inproceedings{10.1145/3630106.3658967,
author = {Moayeri, Mazda and Tabassi, Elham and Feizi, Soheil},
title = {WorldBench: Quantifying Geographic Disparities in LLM Factual Recall},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3630106.3658967},
doi = {10.1145/3630106.3658967},
booktitle = {Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1211â€“1228},
numpages = {18},
keywords = {Bias, Factuality, Fairness, Geographic Disparity, Large Language Models},
location = {Rio de Janeiro, Brazil},
series = {FAccT '24}
}
```
